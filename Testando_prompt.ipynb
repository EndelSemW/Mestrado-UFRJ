{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "88bb28c53911433e9f489bf30282ae29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ddb942e0f0014788b1cbb896ebfefb28",
              "IPY_MODEL_0f7738bf4091431c877a317be8e31280",
              "IPY_MODEL_e9eefc3450d34f1e8e1131a19cd424a9"
            ],
            "layout": "IPY_MODEL_e09e8cc4f6d14994b8c4c40f12b64a05"
          }
        },
        "ddb942e0f0014788b1cbb896ebfefb28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e97e7e93dd24c55990431b26b5c5da2",
            "placeholder": "​",
            "style": "IPY_MODEL_fe7d6c06cd9b4534bdcbd9cd41bbe124",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0f7738bf4091431c877a317be8e31280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8a8776fb70b4635ba86a45cc8fc2ee1",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6abc0089f864ac78386a10fa601ba6f",
            "value": 2
          }
        },
        "e9eefc3450d34f1e8e1131a19cd424a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e43244c9b5a84bc78be16f74fca5b932",
            "placeholder": "​",
            "style": "IPY_MODEL_d972ab1cc0804db8890469ea564b271a",
            "value": " 2/2 [00:56&lt;00:00, 26.32s/it]"
          }
        },
        "e09e8cc4f6d14994b8c4c40f12b64a05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e97e7e93dd24c55990431b26b5c5da2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe7d6c06cd9b4534bdcbd9cd41bbe124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8a8776fb70b4635ba86a45cc8fc2ee1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6abc0089f864ac78386a10fa601ba6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e43244c9b5a84bc78be16f74fca5b932": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d972ab1cc0804db8890469ea564b271a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phyTHe_qpIvL",
        "outputId": "1eb96934-4fb4-40cc-e8c5-ffd5fe7caa19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llmlingua in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.30.1)\n",
            "Requirement already satisfied: transformers>=4.26.0 in /usr/local/lib/python3.10/dist-packages (from llmlingua) (4.40.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from llmlingua) (0.30.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from llmlingua) (2.2.1+cu121)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from llmlingua) (0.7.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from llmlingua) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llmlingua) (1.25.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.0->llmlingua) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.0->llmlingua) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.0->llmlingua) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.0->llmlingua) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.0->llmlingua) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.0->llmlingua) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.0->llmlingua) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.0->llmlingua) (0.4.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->llmlingua) (5.9.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->llmlingua) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->llmlingua) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->llmlingua) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->llmlingua) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->llmlingua) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->llmlingua) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->llmlingua) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->llmlingua) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->llmlingua) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->llmlingua) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->llmlingua) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->llmlingua) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->llmlingua) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->llmlingua) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->llmlingua) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->llmlingua) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->llmlingua) (12.4.127)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->llmlingua) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->llmlingua) (1.4.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.0->llmlingua) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.0->llmlingua) (2.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->llmlingua) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->llmlingua) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install llmlingua openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip show llmlingua"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSTjQpTw8AXD",
        "outputId": "c0278ba1-192b-45ae-a14a-e49020c4bebd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: llmlingua\n",
            "Version: 0.2.2\n",
            "Summary: To speed up LLMs' inference and enhance LLM's perceive of key information, compress the prompt and KV-Cache, which achieves up to 20x compression with minimal performance loss.\n",
            "Home-page: https://github.com/microsoft/LLMLingua\n",
            "Author: The LLMLingua team\n",
            "Author-email: hjiang@microsoft.com\n",
            "License: MIT License\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: accelerate, nltk, numpy, tiktoken, torch, transformers\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import llmlingua\n",
        "help(llmlingua.PromptCompressor.compress_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFZfonsy8XDK",
        "outputId": "760b48b7-1587-40bc-e86d-ca0f9290894e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function compress_prompt in module llmlingua.prompt_compressor:\n",
            "\n",
            "compress_prompt(self, context: List[str], instruction: str = '', question: str = '', rate: float = 0.5, target_token: float = -1, iterative_size: int = 200, force_context_ids: List[int] = None, force_context_number: int = None, use_sentence_level_filter: bool = False, use_context_level_filter: bool = True, use_token_level_filter: bool = True, keep_split: bool = False, keep_first_sentence: int = 0, keep_last_sentence: int = 0, keep_sentence_number: int = 0, high_priority_bonus: int = 100, context_budget: str = '+100', token_budget_ratio: float = 1.4, condition_in_question: str = 'none', reorder_context: str = 'original', dynamic_context_compression_ratio: float = 0.0, condition_compare: bool = False, add_instruction: bool = False, rank_method: str = 'llmlingua', concate_question: bool = True, context_segs: List[str] = None, context_segs_rate: List[float] = None, context_segs_compress: List[bool] = None, target_context: int = -1, context_level_rate: float = 1.0, context_level_target_token: int = -1, return_word_label: bool = False, word_sep: str = '\\t\\t|\\t\\t', label_sep: str = ' ', token_to_word: str = 'mean', force_tokens: List[str] = [], force_reserve_digit: bool = False, drop_consecutive: bool = False, chunk_end_tokens: List[str] = ['.', '\\n'], strict_preserve_uncompressed: bool = True)\n",
            "            Compresses the given context.\n",
            "    \n",
            "            Args:\n",
            "                context (List[str]): List of context strings that form the basis of the prompt.\n",
            "                instruction (str, optional): Additional instruction text to be included in the prompt. Default is an empty string.\n",
            "                question (str, optional): A specific question that the prompt is addressing. Default is an empty string.\n",
            "                rate (float, optional): The maximum compression rate target to be achieved. The compression rate is defined\n",
            "                    the same as in paper \"Language Modeling Is Compression\". Delétang, Grégoire, Anian Ruoss, Paul-Ambroise Duquenne,\n",
            "                    Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya et al. \"Language modeling is compression.\"\n",
            "                    arXiv preprint arXiv:2309.10668 (2023):\n",
            "                    .. math::       ext{Compression Rate} = \frac{   ext{Compressed Size}}{  ext{Raw Size}}\n",
            "                    Default is 0.5. The actual compression rate is generally lower than the specified target, but there can be\n",
            "                    fluctuations due to differences in tokenizers. If specified, it should be a float less than or equal\n",
            "                    to 1.0, representing the target compression rate.\n",
            "                target_token (float, optional): The maximum number of tokens to be achieved. Default is -1, indicating no specific target.\n",
            "                    The actual number of tokens after compression should generally be less than the specified target_token, but there can\n",
            "                    be fluctuations due to differences in tokenizers. If specified, compression will be based on the target_token as\n",
            "                    the sole criterion, overriding the ``rate``.\n",
            "                iterative_size (int, optional): The number of tokens to consider in each iteration of compression. Default is 200.\n",
            "                force_context_ids (List[int], optional): List of specific context IDs to always include in the compressed result. Default is None.\n",
            "                force_context_number (int, optional): The number of context sections to forcibly include. Default is None.\n",
            "                use_sentence_level_filter (bool, optional): Whether to apply sentence-level filtering in compression. Default is False.\n",
            "                use_context_level_filter (bool, optional): Whether to apply context-level filtering in compression. Default is True.\n",
            "                use_token_level_filter (bool, optional): Whether to apply token-level filtering in compression. Default is True.\n",
            "                keep_split (bool, optional): Whether to preserve the original separators without compression. Default is False.\n",
            "                keep_first_sentence (int, optional): Number of sentences to forcibly preserve from the start of the context. Default is 0.\n",
            "                keep_last_sentence (int, optional): Number of sentences to forcibly preserve from the end of the context. Default is 0.\n",
            "                keep_sentence_number (int, optional): Total number of sentences to forcibly preserve in the compression. Default is 0.\n",
            "                high_priority_bonus (int, optional): Bonus score for high-priority sentences to influence their likelihood of being retained. Default is 100.\n",
            "                context_budget (str, optional): Token budget for the context-level filtering, expressed as a string to indicate flexibility. Default is \"+100\".\n",
            "                token_budget_ratio (float, optional): Ratio to adjust token budget during sentence-level filtering. Default is 1.4.\n",
            "                condition_in_question (str, optional): Specific condition to apply to question in the context. Default is \"none\".\n",
            "                reorder_context (str, optional): Strategy for reordering context in the compressed result. Default is \"original\".\n",
            "                dynamic_context_compression_ratio (float, optional): Ratio for dynamically adjusting context compression. Default is 0.0.\n",
            "                condition_compare (bool, optional): Whether to enable condition comparison during token-level compression. Default is False.\n",
            "                add_instruction (bool, optional): Whether to add the instruction to the prompt prefix. Default is False.\n",
            "                rank_method (str, optional): Method used for ranking elements during compression. Default is \"llmlingua\".\n",
            "                concate_question (bool, optional): Whether to concatenate the question to the compressed prompt. Default is True.\n",
            "    \n",
            "                target_context (int, optional): The maximum number of contexts to be achieved. Default is -1, indicating no specific target.\n",
            "                context_level_rate (float, optional): The minimum compression rate target to be achieved in context level. Default is 1.0.\n",
            "                context_level_target_token (float, optional): The maximum number of tokens to be achieved in context level compression.\n",
            "                    Default is -1, indicating no specific target. Only used in the coarse-to-fine compression senario.\n",
            "                force_context_ids (List[int], optional): List of specific context IDs to always include in the compressed result. Default is None.\n",
            "                return_word_label (bool, optional): Whether to return word with corresponding label. Default is False.\n",
            "                word_sep (str, optional): The sep token used in fn_labeled_original_prompt to partition words. Default is \"         |               \".\n",
            "                label_sep (str, optional): The sep token used in fn_labeled_original_prompt to partition word and label.  Default is \" \".\n",
            "                token_to_word (str, optional): How to convert token probability to word probability. Default is \"mean\".\n",
            "                force_tokens (List[str], optional): List of specific tokens to always include in the compressed result. Default is [].\n",
            "                force_reserve_digit  (bool, optional): Whether to forcibly reserve tokens that containing digit (0,...,9). Default is False.\n",
            "                drop_consecutive (bool, optinal): Whether to drop tokens which are in 'force_tokens' but appears consecutively in compressed prompt.\n",
            "                    Default is False.\n",
            "                chunk_end_tokens (List[str], optinal): The early stop tokens for segmenting chunk. Default is [\".\", \"\n",
            "    \"],\n",
            "            Returns:\n",
            "                dict: A dictionary containing:\n",
            "                    - \"compressed_prompt\" (str): The resulting compressed prompt.\n",
            "                    - \"compressed_prompt_list\" (List[str]): List of the resulting compressed prompt. Only used in llmlingua2.\n",
            "                    - \"fn_labeled_original_prompt\" (str): original words along with their labels\n",
            "                        indicating whether to reserve in compressed prompt, in the format (word label_sep label)\n",
            "                        Only used in llmlingua2 when return_word_label = True.\n",
            "                    - \"origin_tokens\" (int): The original number of tokens in the input.\n",
            "                    - \"compressed_tokens\" (int): The number of tokens in the compressed output.\n",
            "                    - \"ratio\" (str): The compression ratio achieved, calculated as the original token number divided by the token number after compression.\n",
            "                    - \"rate\" (str): The compression rate achieved, in a human-readable format.\n",
            "                    - \"saving\" (str): Estimated savings in GPT-4 token usage.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "CF9i6JgjqN-a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Tweets Sentiment Analysis.csv')"
      ],
      "metadata": {
        "id": "_4MEA0J5_6lG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import openai\n",
        "\n",
        "secret_key = getpass.getpass('Digite sua chave da OpenAI: ')\n",
        "openai.api_key = secret_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ungGdQRap9KX",
        "outputId": "061e21bd-9f8a-450b-e3bf-9bec45fb77d3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Digite sua chave da OpenAI: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = openai.Client(api_key=secret_key)"
      ],
      "metadata": {
        "id": "RLvSy22T-u1K"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_tokens(text):\n",
        "    return len(text.split())"
      ],
      "metadata": {
        "id": "B20YuybSLOAT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment_openai(text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a sentiment analysis expert. Please classify the tweet as 'positive', 'negative', or 'neutral'. Do not provide any other form of response.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Analyze the sentiment of the following text: '{text}'\"}\n",
        "        ],\n",
        "        max_tokens=300,\n",
        "        temperature=0.3,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    sentiment = response.choices[0].message.content.strip()\n",
        "    tokens = count_tokens(f\"Analyze the sentiment of the following text: '{text}'\")\n",
        "    return sentiment, tokens"
      ],
      "metadata": {
        "id": "eHcJqY83qm8c"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llmlingua import PromptCompressor"
      ],
      "metadata": {
        "id": "K3ifnmApwx6Q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_lingua = PromptCompressor()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312,
          "referenced_widgets": [
            "88bb28c53911433e9f489bf30282ae29",
            "ddb942e0f0014788b1cbb896ebfefb28",
            "0f7738bf4091431c877a317be8e31280",
            "e9eefc3450d34f1e8e1131a19cd424a9",
            "e09e8cc4f6d14994b8c4c40f12b64a05",
            "5e97e7e93dd24c55990431b26b5c5da2",
            "fe7d6c06cd9b4534bdcbd9cd41bbe124",
            "c8a8776fb70b4635ba86a45cc8fc2ee1",
            "a6abc0089f864ac78386a10fa601ba6f",
            "e43244c9b5a84bc78be16f74fca5b932",
            "d972ab1cc0804db8890469ea564b271a"
          ]
        },
        "id": "bslNoE7P8aZJ",
        "outputId": "7ce1a75a-a949-4fd1-ff07-2148502bf0c6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88bb28c53911433e9f489bf30282ae29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment_llmlingua(text):\n",
        "    context = [f\"Analyze the sentiment of the following text: '{text}'\"]\n",
        "    instruction = \"You are a sentiment analysis expert. Please classify the text as 'positive', 'negative', or 'neutral'. Do not provide any other form of response.\"\n",
        "\n",
        "    compressed_prompt = llm_lingua.compress_prompt(\n",
        "        context=context,\n",
        "        instruction=instruction,\n",
        "        question=\"\",\n",
        "        rate=0.5,  # Taxa de compressão desejada (50%)\n",
        "        target_token=100  # Reduzindo o número de tokens alvo\n",
        "    )\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a sentiment analysis expert. Please classify the tweet as 'positive', 'negative', or 'neutral'. Do not provide any other form of response.\"},\n",
        "            {\"role\": \"user\", \"content\": compressed_prompt['compressed_prompt']}\n",
        "        ],\n",
        "        max_tokens=300,\n",
        "        temperature=0.3,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    sentiment = response.choices[0].message.content.strip()\n",
        "    tokens = count_tokens(response.choices[0].message.content.strip())\n",
        "    return sentiment, tokens, compressed_prompt"
      ],
      "metadata": {
        "id": "cME5HkPwqtza"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df['text'].tolist()\n",
        "original_sentiments = df['sentiment'].tolist()\n",
        "\n",
        "# Analisando com OpenAI\n",
        "results_openai = [analyze_sentiment_openai(text) for text in texts]\n",
        "\n",
        "# Analisando com LLMLingua\n",
        "results_llmlingua = [analyze_sentiment_llmlingua(text) for text in texts]\n",
        "\n",
        "# Criando um DataFrame para comparação\n",
        "df_results = pd.DataFrame({\n",
        "    'Text': texts,\n",
        "    'Original_sentiment': original_sentiments,\n",
        "    'Sentiment_OpenAI': [result[0] for result in results_openai],\n",
        "    'Tokens_OpenAI': [result[1] for result in results_openai],\n",
        "    'Sentiment_LLMLingua': [result[0] for result in results_llmlingua],\n",
        "    'Tokens_LLMLingua': [result[1] for result in results_llmlingua]\n",
        "})\n",
        "\n",
        "df_results.to_csv('results.csv', index=False)"
      ],
      "metadata": {
        "id": "zgdoNvwBq0Nj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ZlW7LQmY-uYJ",
        "outputId": "b3cd6beb-88a4-411a-8262-7fa68423db44"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Text Original_sentiment  \\\n",
              "0                           i checked. we didn`t win            neutral   \n",
              "1  sitting here, waiting to go to the gym.....alm...            neutral   \n",
              "2          i feel sorry for my bestie good luck ali!            neutral   \n",
              "3  airsoft is so much fun! i play with my brother...           positive   \n",
              "4   alice doesn`t know what to wear to the cinema <3           negative   \n",
              "\n",
              "  Sentiment_OpenAI  Tokens_OpenAI Sentiment_LLMLingua  Tokens_LLMLingua  \n",
              "0         negative             12            negative                 1  \n",
              "1         Negative             29            Negative                 1  \n",
              "2         positive             16            positive                 1  \n",
              "3         Positive             23            Positive                 1  \n",
              "4          Neutral             17             Neutral                 1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-66fa477f-3445-4b0d-ad0c-7fd10ca46eb1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Original_sentiment</th>\n",
              "      <th>Sentiment_OpenAI</th>\n",
              "      <th>Tokens_OpenAI</th>\n",
              "      <th>Sentiment_LLMLingua</th>\n",
              "      <th>Tokens_LLMLingua</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i checked. we didn`t win</td>\n",
              "      <td>neutral</td>\n",
              "      <td>negative</td>\n",
              "      <td>12</td>\n",
              "      <td>negative</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sitting here, waiting to go to the gym.....alm...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>Negative</td>\n",
              "      <td>29</td>\n",
              "      <td>Negative</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i feel sorry for my bestie good luck ali!</td>\n",
              "      <td>neutral</td>\n",
              "      <td>positive</td>\n",
              "      <td>16</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>airsoft is so much fun! i play with my brother...</td>\n",
              "      <td>positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>23</td>\n",
              "      <td>Positive</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>alice doesn`t know what to wear to the cinema &lt;3</td>\n",
              "      <td>negative</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>17</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66fa477f-3445-4b0d-ad0c-7fd10ca46eb1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-66fa477f-3445-4b0d-ad0c-7fd10ca46eb1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-66fa477f-3445-4b0d-ad0c-7fd10ca46eb1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-191b517f-d05e-46b2-939a-09af31943148\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-191b517f-d05e-46b2-939a-09af31943148')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-191b517f-d05e-46b2-939a-09af31943148 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 200,\n  \"fields\": [\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 200,\n        \"samples\": [\n          \"looking at our photo shoot pics...check out our myspace pics to see them...i love them all...\",\n          \"_d you`re gonna take good care of that little baby and he`s gonna be a strong boy, that`s for sure\",\n          \"trying to find someone to go to the mall with.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Original_sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"neutral\",\n          \"positive\",\n          \"negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment_OpenAI\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"The sentiment of the text is 'neutral'.\",\n          \"Negative\",\n          \"neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tokens_OpenAI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 8,\n        \"max\": 36,\n        \"num_unique_values\": 28,\n        \"samples\": [\n          14,\n          34,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment_LLMLingua\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"negative\",\n          \"Negative\",\n          \"neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tokens_LLMLingua\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}